{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Short cron version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import numpy as np\n",
    "import re\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.max_colwidth', -1)\n",
    "import datetime\n",
    "import time\n",
    "from random import randint\n",
    "import os\n",
    "import glob\n",
    "import smtplib\n",
    "from calendar import monthrange"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "request_headers = {\n",
    "    \"method\": \"GET\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/61.0.3163.100 Safari/537.36\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete \\n in strings\n",
    "\n",
    "def clean_string(string):\n",
    "    string = re.sub(\"\\n \",\"\", string)\n",
    "    string = re.sub(\"\\n\",\"\", string)\n",
    "    return string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract date format from date + id string\n",
    "\n",
    "def get_date(string):\n",
    "    match = re.search('\\d\\d\\.\\d\\d\\.\\d{4}', string)\n",
    "    #date = datetime.datetime.strptime(match.group(), '%d.%m.%Y').date()\n",
    "    date = pd.to_datetime(match.group(), format = \"%d.%m.%Y\")\n",
    "\n",
    "    return date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataframe with all url, their date, ID and court information (while avoiding a bunch of scraping errors)\n",
    "\n",
    "\n",
    "def get_url(text):\n",
    "\n",
    "    rulings = []\n",
    "    \n",
    "    # Si on trouve des urls\n",
    "    if len(text.findAll('li')) > 0:\n",
    "\n",
    "        # On passe a travers et on les enregistre\n",
    "        for li in text.findAll('li'):\n",
    "            ruling = {}\n",
    "\n",
    "            #Get date & ID code\n",
    "\n",
    "            if li.find('span', {'class': 'rank_title'}) is None:\n",
    "                print(\"Date/ID code error found\")\n",
    "                ruling['date'] = np.nan\n",
    "                ruling['date_id'] = np.nan\n",
    "                ruling['date'] = np.nan\n",
    "                ruling['id_code'] = np.nan\n",
    "\n",
    "            else:\n",
    "                rank_title = li.find('span', {'class': 'rank_title'})\n",
    "                # Get the string containing date+ID\n",
    "                ruling['date_id'] = clean_string(rank_title.text)\n",
    "                # Get date alone\n",
    "                ruling['date'] = get_date(ruling['date_id'])\n",
    "                ruling['year'] = pd.to_datetime(ruling['date']).year\n",
    "\n",
    "\n",
    "                if ruling['date_id'].find('\\d\\d\\.\\d\\d\\.\\d{4}') is None:\n",
    "                    print(\"Date/ID code format error found in: \", ruling['date_id'])\n",
    "                    ruling['id_code'] = np.nan\n",
    "                else:\n",
    "                    #Get ID code alone\n",
    "                    ruling['id_code'] = re.sub('\\d\\d\\.\\d\\d\\.\\d{4} ', '', ruling['date_id'])\n",
    "\n",
    "            # Get URL\n",
    "\n",
    "            if li.find('a').get('href') is None:\n",
    "                print(\"URL error found in:\", ruling['date_id'])\n",
    "                ruling['url'] = np.nan\n",
    "\n",
    "            else: \n",
    "                ruling['url'] = li.find('a').get('href')\n",
    "\n",
    "\n",
    "            # Get court, subject and object information\n",
    "\n",
    "            if li.find('div', {'class': 'rank_data'}) is None:\n",
    "                print(\"Court/Subject/Object error found\")\n",
    "                ruling['court'] = np.nan\n",
    "                ruling['subject'] = np.nan\n",
    "                ruling['object'] = np.nan\n",
    "\n",
    "            else: \n",
    "\n",
    "                rank_data = li.find('div', {'class': 'rank_data'})\n",
    "\n",
    "                if rank_data.find('div', {'class': 'court small normal'}) is None:\n",
    "                    ruling['court'] = np.nan \n",
    "                else:\n",
    "                    ruling['court'] = clean_string(rank_data.find('div', {'class': 'court small normal'}).text)\n",
    "\n",
    "                if rank_data.find('div', {'class': 'subject small normal'}) is None:\n",
    "                    ruling['subject'] = np.nan\n",
    "                else:\n",
    "                    ruling['subject'] = clean_string(rank_data.find('div', {'class': 'subject small normal'}).text)\n",
    "\n",
    "                if rank_data.find('div', {'class': 'object small normal'}) is None:\n",
    "                        ruling['object'] = np.nan\n",
    "                else:\n",
    "                    ruling['object'] = clean_string(rank_data.find('div', {'class': 'object small normal'}).text)\n",
    "\n",
    "\n",
    "            rulings.append(ruling)\n",
    "            df_rulings = pd.DataFrame(rulings)\n",
    "\n",
    "    # Si on ne trouve pas d'urls\n",
    "    else:\n",
    "        columns = ['court', 'date', 'date_id', 'id_code', 'object', 'subject', 'url', 'year']\n",
    "        \n",
    "        df_rulings = pd.DataFrame(columns=columns)\n",
    "\n",
    "\n",
    "    return df_rulings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save evolution of rulings over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117989\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Find number of result pages today\n",
    "\n",
    "search_url = \"https://www.bger.ch/ext/eurospider/live/fr/php/aza/http/index.php?lang=fr&type=simple_query&query_words=&lang=fr&top_subcollection_aza=all&from_date=&to_date=&x=29&y=14\"\n",
    "html = requests.get(search_url, headers=request_headers)\n",
    "\n",
    "connection_attempts = 0\n",
    "while(True):\n",
    "    if html.status_code == 200:\n",
    "        break\n",
    "    else:\n",
    "        connection_attempts +=1\n",
    "\n",
    "    if connection_attempts > 10:\n",
    "        print(\"CONNECTION ERROR\")\n",
    "        break\n",
    "soup = BeautifulSoup(html.text, \"html5lib\")\n",
    "page_header = soup.find('div', {'class': 'ranklist_header center'})\n",
    "\n",
    "total_pages = re.search('\\d{6}', page_header.text).group(0)\n",
    "print(total_pages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initiate log file with scrape date and number of rulings on site \n",
    "# ! Run once then comment this cell ! \n",
    "\n",
    "#page_increase = pd.DataFrame([{\n",
    "#    'scrape_date': datetime.datetime.today().strftime('%Y-%m-%d-%H-%M'), \n",
    "#    'page_total': total_pages,\n",
    "#    'id': scrape_id\n",
    "#}])  \n",
    "\n",
    "#page_increase.to_csv(\"urls/logs/page_increase_doc.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "page_increase_doc = pd.read_csv(\"urls/logs/page_increase_doc.csv\")\n",
    "scrape_id = len(page_increase_doc)\n",
    "\n",
    "page_increase = pd.DataFrame([{\n",
    "    'scrape_date': datetime.datetime.today().strftime('%Y-%m-%d-%H-%M'), \n",
    "    'page_total': total_pages,\n",
    "    'id': scrape_id\n",
    "}])                \n",
    "\n",
    "# Open and update document\n",
    "update_page_increase_doc = page_increase_doc.append(page_increase)\n",
    "update_page_increase_doc.to_csv('urls/logs/page_increase_doc.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Scrape and save each month in a csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create today's folder to store scraped files (run once only)\n",
    "\n",
    "todays_folder = datetime.datetime.today().strftime('%Y-%m-%d')\n",
    "os.makedirs('urls/'+ todays_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MONTH: 5 / 2018\n",
      "page_number_start 1\n",
      "page_number_start 2\n",
      "Pas d'elements dans la page, on sauve le mois\n",
      "Taille du df 20\n",
      "MONTH: 6 / 2018\n",
      "page_number_start 1\n",
      "page_number_start 2\n",
      "Pas d'elements dans la page, on sauve le mois\n",
      "Taille du df 20\n",
      "MONTH: 7 / 2018\n",
      "page_number_start 1\n",
      "Pas d'elements dans la page, on sauve le mois\n",
      "Taille du df 0\n"
     ]
    }
   ],
   "source": [
    "# Loop through search results page for each year\n",
    "for year in range(2000, int(datetime.datetime.today().strftime('%Y'))+1):\n",
    "    \n",
    "    year_int = year\n",
    "    year_str = str(year)\n",
    "    \n",
    "    # Loop through search results page for each month\n",
    "    for month in range(1,13):\n",
    "        \n",
    "        print(\"MONTH:\", month, \"/\",year)\n",
    "                \n",
    "        # monthrange(2011, 2) returns weekday of the first day of the month and number of days in the month\n",
    "        end_month_str = str(monthrange(year, month)[1])\n",
    "        \n",
    "        # Make sure all months numbers have two digits\n",
    "        formatted_month_str = str('%02d' % month)\n",
    "        \n",
    "        # Loop through search results page\n",
    "        page_number = 1\n",
    "        while(True):\n",
    "            try:\n",
    "                print('page_number_start', str(page_number))\n",
    "                url = 'https://www.bger.ch/ext/eurospider/live/fr/php/aza/http/index.php?lang=fr&type=simple_query&page='+str(page_number)+'&from_date=01.'+formatted_month_str+'.'+year_str+'&to_date='+end_month_str+'.'+formatted_month_str+'.'+year_str+'&sort=relevance&insertion_date=&top_subcollection_aza=all&query_words='\n",
    "                        \n",
    "                connection_attempts = 0\n",
    "                # Loop until connection works\n",
    "                \n",
    "                while(True):\n",
    "                    \n",
    "                    html = requests.get(url, headers=request_headers)\n",
    "                    if html.status_code == 200:\n",
    "                        break\n",
    "                    else:\n",
    "                        connection_attempts +=1\n",
    "                        if connection_attempts > 10:\n",
    "                            print(\"CONNECTION ERROR\")\n",
    "                        \n",
    "                            # Email notification\n",
    "                            server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "                            server.starttls()\n",
    "                            server.login(\"fanny.giroud@24heures.ch\", \"6v7pl7S8Fr\")\n",
    "\n",
    "                            msg = \"ERROR #1 while scraping: connection for request failed after 10 attempts\"\n",
    "                            server.sendmail(\"fanny.giroud@24heures.ch\", \"fanny.giroud@24heures.ch\", msg)\n",
    "                            server.quit()\n",
    "                        \n",
    "                            break\n",
    "                \n",
    "                # Recupere la div qui contient les urls\n",
    "                soup = BeautifulSoup(html.text, \"html5lib\")\n",
    "                rank_soup = soup.find('div', {'class': 'ranklist_content'})\n",
    "            \n",
    "                # Si on est sur la premiere page il faut creer une df\n",
    "                if page_number == 1:\n",
    "                    urls = get_url(rank_soup)\n",
    "                    \n",
    "                # Sinon, la concaténer avec les nouveaux résultats\n",
    "                else:\n",
    "                    urls = pd.concat([urls, get_url(rank_soup)])\n",
    "\n",
    "                # Add year in dataframe    \n",
    "                urls['year'] = year\n",
    "                urls['month'] = month\n",
    "           \n",
    "                \n",
    "                # Si on ne trouve plus rien sur la page, on a fini, on peut sauver le mois\n",
    "                if len(rank_soup.findAll('li')) == 0 :\n",
    "                    \n",
    "                    print(\"Pas d'elements dans la page, on sauve le mois\")\n",
    "                    print(\"Taille du df\", len(urls))\n",
    "                    \n",
    "                    # Parsed all the pages, getting an empty one and saving\n",
    "                    urls.to_csv(\"urls/\"+ todays_folder + '/' + datetime.datetime.today().strftime('%Y-%m-%d')+ \"_scraping_\" + str(month) + \"_\" + str(year) + \".csv\", index = False)\n",
    "                    \n",
    "                    # On sort de la boucle du mois\n",
    "                    break\n",
    "                \n",
    "                # Si on trouve encore quelque chose sur la page\n",
    "                else:\n",
    "                    # On fait une pause avant de charger la page suivante\n",
    "                    time.sleep(randint(1, 4))\n",
    "\n",
    "                    page_number += 1\n",
    "\n",
    "    \n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"-------UNEXPECTED ERROR------------\")\n",
    "                break\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat scraped urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = glob.glob('urls/'+todays_folder + \"/*.csv\")\n",
    "list_of_dfs = [pd.read_csv(filename) for filename in filenames]\n",
    "\n",
    "combined_df = pd.concat(list_of_dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save concat ok\n"
     ]
    }
   ],
   "source": [
    "combined_df.reset_index(drop=True).to_csv('urls/concats/' + todays_folder +'.csv', index = False)\n",
    "print('save concat ok')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, b'2.0.0 closing connection d18-v6sm2322550eds.40 - gsmtp')"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Email notifications\n",
    "\n",
    "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "server.starttls()\n",
    "server.login(\"fanny.giroud@24heures.ch\", \"6v7pl7S8Fr\")\n",
    "\n",
    "msg = \"Scrape of the day ok. URLs list is updated\"\n",
    "server.sendmail(\"fanny.giroud@24heures.ch\", \"fanny.giroud@24heures.ch\", msg)\n",
    "server.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
